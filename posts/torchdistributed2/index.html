<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  
    <title>Distributed Training 2 (FSDP) :: Narain</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="In the ever-evolving landscape of artificial intelligence and machine learning, the demand for training larger and more complex models continues to grow. Traditional techniques such as data parallelism, model parallelism, and tensor parallelism have played crucial roles in enabling this growth by distributing the training workload across multiple GPUs and machines. These methods are foundational in making large-scale training feasible by addressing issues related to computation and memory distribution.
Where did all the memory go?" />
<meta name="keywords" content=", " />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="//localhost:1313/posts/torchdistributed2/" />


  






  
  
  
  
  
  <link rel="stylesheet" href="//localhost:1313/styles.css">







  <link rel="shortcut icon" href="//localhost:1313/img/theme-colors/pink.png">
  <link rel="apple-touch-icon" href="//localhost:1313/img/theme-colors/pink.png">



<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Distributed Training 2 (FSDP)">
<meta property="og:description" content="In the ever-evolving landscape of artificial intelligence and machine learning, the demand for training larger and more complex models continues to grow. Traditional techniques such as data parallelism, model parallelism, and tensor parallelism have played crucial roles in enabling this growth by distributing the training workload across multiple GPUs and machines. These methods are foundational in making large-scale training feasible by addressing issues related to computation and memory distribution.
Where did all the memory go?" />
<meta property="og:url" content="//localhost:1313/posts/torchdistributed2/" />
<meta property="og:site_name" content="Narain" />

  
  
  <meta property="og:image" content="//localhost:1313/">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">


  <meta property="article:published_time" content="2024-06-20 01:10:00 -0700 MST" />












</head>
<body class="pink">


<div class="container center headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Narain
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="https://narain1.netlify.app">Showcase</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/about" >About</a></li>
        
      
        
          <li><a href="https://narain1.netlify.app" >Showcase</a></li>
        
      
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<article class="post">
  <h1 class="post-title">
    <a href="//localhost:1313/posts/torchdistributed2/">Distributed Training 2 (FSDP)</a>
  </h1>
  <div class="post-meta"><time class="post-date">2024-06-20</time></div>

  
    <span class="post-tags">
      
      #<a href="//localhost:1313/tags/"></a>&nbsp;
      
      #<a href="//localhost:1313/tags/"></a>&nbsp;
      
    </span>
  
  


  

  <div class="post-content"><div>
        <p>In the ever-evolving landscape of artificial intelligence and machine learning, the
demand for training larger and more complex models continues to grow. Traditional
techniques such as data parallelism, model parallelism, and tensor parallelism have
played crucial roles in enabling this growth by distributing the training workload
across multiple GPUs and machines. These methods are foundational in making large-scale
training feasible by addressing issues related to computation and memory distribution.</p>
<h2 id="where-did-all-the-memory-go">Where did all the memory go?<a href="#where-did-all-the-memory-go" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<ul>
<li>activation recomputation</li>
<li>parameter buffer optimizer</li>
<li>second order statistics</li>
</ul>
<p>Despite all the efforts we would still face out of memory issues when training a llama 70B parameter
model with a batch size of 1. Model training uses lot of memory when training with conventional
training methods where all parameters of model and optimizers are saved locally to each GPUs global
memory. When we train deep learning models we have multiple copies of the models for parameters and optimizers
gradients and optimizer features like momemntum which save past gradients weighted average. These models compute
buffer adds on to the overhead of training models efficiently. This is where DeepSpeed and its ZeRO
(Zero Redundancy Optimizer) technology come into play.</p>
<h2 id="special-offering-by-deepspeed-zero">Special offering by deepspeed zero<a href="#special-offering-by-deepspeed-zero" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>At the heart of ZeRO optimizer, which addresses the limitations of traditional
data, model, and tensor parallelism techniques. ZeRO is designed to maximize memory efficiency
and scalability, enabling the training of models with hundreds of billions of parameters on existing hardware.</p>

  <figure class="center" >
    <img src="/images/fsdp1.png"  alt="fully sharded data parallel"   style="border-radius: 8px;"  />
    
      <figcaption class="center" >fully sharded data parallel</figcaption>
    
  </figure>


<p>As its name suggests zero stands for zero redundancy of parameters, unlike conventional training scheme where
each GPU maintains a replica of the model. Zero ensures that the entire cluster shares a single
copy of the model. Paramters required for computation are fetched from the respective devices
using broadcast operations. This allows for data parallel operations on large batch sizes than typically
possible. followed by an all-reduce operaation back to the device responsible for that part of the weight.
This strategy of sharing weights among GPUs essentially increases the communication required but the
latency for these fetch operations can be overlapped with computation operations as most of the
deep learning operations happen in linear fashion.</p>
<p>Zero has three different implementations based on the parameters it includes in the sharding</p>
<ul>
<li>zero 1 - sharding optimizer states</li>
<li>zero 2 - sharding gradients + optimizer</li>
<li>zero 3 - sharding gradients + optimizer + paramters</li>
</ul>

      </div></div>

  
    
<div class="pagination">
    <div class="pagination__title">
        <span class="pagination__title-h">Read other posts</span>
        <hr />
    </div>
    <div class="pagination__buttons">
        
        
        <span class="button next">
            <a href="//localhost:1313/posts/torchdistributed/">
                <span class="button__text">Distributed Training</span>
                <span class="button__icon">→</span>
            </a>
        </span>
        
    </div>
</div>

  

  
    

  
</article>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2024 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/bundle.min.js"></script>





  
</div>

</body>
</html>
