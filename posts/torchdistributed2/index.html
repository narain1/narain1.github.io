<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  
    <title>Distributed Training 2 (FSDP) :: Narain</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="In the ever-evolving landscape of artificial intelligence and machine learning, the demand for training larger and more complex models continues to grow. Traditional techniques such as data parallelism, model parallelism, and tensor parallelism have played crucial roles in enabling this growth by distributing the training workload across multiple GPUs and machines. These methods are foundational in making large-scale training feasible by addressing issues related to computation and memory distribution.
Memory Consumption in Neural Networks: Where Did All the Memory Go?" />
<meta name="keywords" content=", " />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="//localhost:1313/posts/torchdistributed2/" />


  





  
  <link rel="stylesheet" href="//localhost:1313/css/buttons.min.2bc533403a27dfe0e93105a92502b42ce4587e2e4a87d9f7d349e51e16e09478.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/code.min.00125962708925857e7b66dbc58391d55be1191a3d0ce2034de8c9cd2c481c36.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/fonts.min.90c955c31dd7c0e05aae3d4f583d4d8a2af799d69c961337eaf2a825063a55dd.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/footer.min.2e3eb191baee58dd05a9f0104ac1fab0827bca7c64dafe0b2579f934c33a1d69.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/header.min.b6fb4423cf82a9f9d7abc9cd010223fa3d70a6526a3f28f8e17d814c06e18f9e.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/main.min.1d8be2dd1b5de9fdaed058c8c59fcf4485f36619574abfb47ed0cfda4812c16d.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/menu.min.83637a90d903026bc280d3f82f96ceb06c5fc72b7c1a8d686afb5bbf818a29f7.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/pagination.min.82f6400eae7c7c6dc3c866733c2ec0579e4089608fea69400ff85b3880aa0d3c.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/post.min.fc74ca360273c1d828da3c02b8174eba435607b369d98418ccc6f2243cd4e75d.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/prism.min.9023bbc24533d09e97a51a0a42a5a7bfe4c591ae167c5551fb1d2191d11977c0.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/syntax.min.cc789ed9377260d7949ea4c18781fc58959a89287210fe4edbff44ebfc1511b6.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/terminal.min.0b4ff4e0376896fa06ce9762c55cfe3b00fa64e8324a8fc5e3417945df607f7a.css">

  
  <link rel="stylesheet" href="//localhost:1313/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css">







<link rel="shortcut icon" href="//localhost:1313/favicon.png">
<link rel="apple-touch-icon" href="//localhost:1313/apple-touch-icon.png">


<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Distributed Training 2 (FSDP)">
<meta property="og:description" content="In the ever-evolving landscape of artificial intelligence and machine learning, the demand for training larger and more complex models continues to grow. Traditional techniques such as data parallelism, model parallelism, and tensor parallelism have played crucial roles in enabling this growth by distributing the training workload across multiple GPUs and machines. These methods are foundational in making large-scale training feasible by addressing issues related to computation and memory distribution.
Memory Consumption in Neural Networks: Where Did All the Memory Go?" />
<meta property="og:url" content="//localhost:1313/posts/torchdistributed2/" />
<meta property="og:site_name" content="Narain" />

  
  
  <meta property="og:image" content="//localhost:1313/">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">


  <meta property="article:published_time" content="2024-06-20 01:10:00 -0700 MST" />












</head>
<body>


<div class="container center">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Narain
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="https://narain1.netlify.app">portfolio</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/about" >About</a></li>
        
      
        
          <li><a href="https://narain1.netlify.app" >portfolio</a></li>
        
      
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<article class="post">
  <h1 class="post-title">
    <a href="//localhost:1313/posts/torchdistributed2/">Distributed Training 2 (FSDP)</a>
  </h1>
  <div class="post-meta"><time class="post-date">2024-06-20</time></div>

  
    <span class="post-tags">
      
      #<a href="//localhost:1313/tags/machine-learning/">machine learning</a>&nbsp;
      
      #<a href="//localhost:1313/tags/"></a>&nbsp;
      
    </span>
  
  


  

  <div class="post-content"><div>
        <p>In the ever-evolving landscape of artificial intelligence and machine learning, the
demand for training larger and more complex models continues to grow. Traditional
techniques such as data parallelism, model parallelism, and tensor parallelism have
played crucial roles in enabling this growth by distributing the training workload
across multiple GPUs and machines. These methods are foundational in making large-scale
training feasible by addressing issues related to computation and memory distribution.</p>
<h2 id="memory-consumption-in-neural-networks-where-did-all-the-memory-go">Memory Consumption in Neural Networks: Where Did All the Memory Go?<a href="#memory-consumption-in-neural-networks-where-did-all-the-memory-go" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>When training large neural network models like GPT-2, understanding memory consumption
is crucial. Let&rsquo;s breakdown where all the memory goes and how different techniques can help
manage it more efficiently.</p>
<h3 id="model-weights">Model Weights<a href="#model-weights" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>The essential parameter for any neural network it the weights required to compute the
forward pass. Model weights can be stored in multiple formats including</p>
<ul>
<li><strong>single precision (fp32)</strong>: Typically used for high accuracy.</li>
<li><strong>half precision</strong>: Used to reduce memory consumption and computational cost</li>
</ul>
<h3 id="model-inference">Model Inference<a href="#model-inference" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>During inference, the memory consumption is primarily due to:</p>
<ul>
<li><strong>Model memory</strong>: The storage required for model weights.</li>
<li><strong>Activation memory</strong>: The intermediate values computed during the forward pass.</li>
</ul>
<h3 id="model-training">Model training<a href="#model-training" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>Training a model involves additional memory overhead due to:</p>
<ul>
<li><strong>gradient Memory</strong>: storage for gradients computed during backpropagation.</li>
<li><strong>optimizer memory</strong>: The intermediate values computed during the forward pass.</li>
</ul>
<h3 id="optimizer-memory-requirements">Optimizer memory requirements<a href="#optimizer-memory-requirements" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>Different optimizers have different memory requirements.</p>
<ul>
<li><strong>SGD (Stochastic Gradient Descent)</strong>:  requires no additional memory beyond model weights.</li>
<li><strong>SGD with momentum</strong>: Requires additional memory equal to the size of the weights.</li>
<li><strong>Adam</strong>: Requires twice the memory of the weights (for maintaining both the moment estimates).</li>
</ul>
<h3 id="techniques-for-reducing-memory-usage">Techniques for reducing Memory usage.<a href="#techniques-for-reducing-memory-usage" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>To mitigate high memory consumptioon, several techniques can be employed.</p>
<h4 id="activation-recomputation">Activation Recomputation<a href="#activation-recomputation" class="hanchor" ariaLabel="Anchor">#</a> </h4>
<p>This technique involves recomputing activations during the backward pass instead of saving them in memory.
It can significantly reduce activation memory usage, although it may increase computational cost.</p>



  <div class="collapsable-code">
    <input id="1" type="checkbox" checked />
    <label for="1">
      <span class="collapsable-code__language">python</span>
      <span class="collapsable-code__title">Activation Recomputation in Linear Layer</span>
      <span class="collapsable-code__toggle" data-label-expand="show" data-label-collapse="hide"></span>
    </label>
    <pre class="language-python" ><code>
import torch
from torch.autograd import Function

class LinearFunction(Function):
    @staticmethod
    def forward(ctx, input, weight, bias=None):
        ctx.save_for_backward(input, weight)
        output = input.mm(weight.t())
        if bias is not None:
            output &#43;= bias.unsqueeze(0).expand_as(output)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        input, weight = ctx.saved_tensors
        grad_input = grad_weight = grad_bias = None

        if ctx.needs_input_grad[0]:
            grad_input = grad_output.mm(weight)
        if ctx.needs_input_grad[1]:
            grad_weight = grad_output.t().mm(input)
        if bias is not None and ctx.needs_input_grad[2]:
            grad_bias = grad_output.sum(0)

        return grad_input, grad_weight, grad_bias

class Linear(torch.nn.Module):
    def __init__(self, input_features, output_features, bias=True):
        super(Linear, self).__init__()
        self.input_features = input_features
        self.output_features = output_features

        # Initialize the weight and bias parameters
        self.weight = torch.nn.Parameter(torch.Tensor(output_features, input_features))
        if bias:
            self.bias = torch.nn.Parameter(torch.Tensor(output_features))
        else:
            self.register_parameter(&#39;bias&#39;, None)

    def forward(self, input):
        return LinearFunction.apply(input, self.weight, self.bias)

</code></pre>
  </div>


<ul>
<li>parameter buffer optimizer</li>
<li>second order statistics</li>
</ul>
<p>Despite all the efforts we would still face out of memory issues when training a llama 70B parameter
model with a batch size of 1. Model training uses lot of memory when training with conventional
training methods where all parameters of model and optimizers are saved locally to each GPUs global
memory. When we train deep learning models we have multiple copies of the models for parameters and optimizers
gradients and optimizer features like momemntum which save past gradients weighted average. These models compute
buffer adds on to the overhead of training models efficiently. This is where DeepSpeed and its ZeRO
(Zero Redundancy Optimizer) technology come into play.</p>
<h2 id="special-offering-by-deepspeed-zero">Special offering by deepspeed zero<a href="#special-offering-by-deepspeed-zero" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>At the heart of ZeRO optimizer, which addresses the limitations of traditional
data, model, and tensor parallelism techniques. ZeRO is designed to maximize memory efficiency
and scalability, enabling the training of models with hundreds of billions of parameters on existing hardware.</p>

  <figure class="center" >
    <img src="/images/fsdp1.png"  alt="fully sharded data parallel"   style="border-radius: 8px;"  />
    
      <figcaption class="center" >fully sharded data parallel</figcaption>
    
  </figure>


<p>As its name suggests zero stands for zero redundancy of parameters, unlike conventional training scheme where
each GPU maintains a replica of the model. Zero ensures that the entire cluster shares a single
copy of the model. Paramters required for computation are fetched from the respective devices
using broadcast operations. This allows for data parallel operations on large batch sizes than typically
possible. followed by an all-reduce operaation back to the device responsible for that part of the weight.
This strategy of sharing weights among GPUs essentially increases the communication required but the
latency for these fetch operations can be overlapped with computation operations as most of the
deep learning operations happen in linear fashion.</p>
<p>Zero has three different stages based on the parameters it includes in the sharding</p>
<blockquote>
<ul>
<li>zero 1 - optimizer</li>
<li>zero 2 - gradients + optimizer</li>
<li>zero 3 - gradients + optimizer + parameters</li>
</ul>
</blockquote>
<p>
  <figure class="center" >
    <img src="/images/fsdp2.png"  alt="fully sharded data parallel"   style="border-radius: 8px;"  />
    
      <figcaption class="center" >fully sharded data parallel</figcaption>
    
  </figure>



  <figure class="center" >
    <img src="/images/fsdp3.png"  alt="fully sharded data parallel"   style="border-radius: 8px;"  />
    
      <figcaption class="center" >fully sharded data parallel</figcaption>
    
  </figure>



  <figure class="center" >
    <img src="/images/fsdp4.png"  alt="fully sharded data parallel"   style="border-radius: 8px;"  />
    
      <figcaption class="center" >fully sharded data parallel</figcaption>
    
  </figure>

</p>



  <div class="collapsable-code">
    <input id="2" type="checkbox" checked />
    <label for="2">
      <span class="collapsable-code__language">python</span>
      <span class="collapsable-code__title">Bert Training using zero Redundancy optimizer pytorch</span>
      <span class="collapsable-code__toggle" data-label-expand="show" data-label-collapse="hide"></span>
    </label>
    <pre class="language-python" ><code>
import torch
from transformers import BertForMaskedLM, BertTokenizerFast
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed.optim import ZeroRedundancyOptimizer

# Initialize tokenizer and model
tokenizer = BertTokenizerFast.from_pretrained(&#39;bert-base-uncased&#39;)
model = BertForMaskedLM.from_pretrained(&#39;bert-base-uncased&#39;)

# Assuming you have your dataset in the form of a list of sentences
sentences = [&#34;Your sentence 1&#34;, &#34;Your sentence 2&#34;, ...]
inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=&#34;pt&#34;)
labels = inputs.input_ids.detach().clone()

# Create a mask for the input ids
rand = torch.rand(labels.shape)
mask_arr = (rand &lt; 0.15) * (inputs.input_ids != tokenizer.pad_token_id)
selection = []
for i in range(labels.shape[0]):
    selection.append(torch.flatten(mask_arr[i].nonzero()).tolist())
for i in range(labels.shape[0]):
    inputs.input_ids[i, selection[i]] = tokenizer.mask_token_id

# Move model and input tensors to the device
device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)
model.to(device)
inputs.input_ids = inputs.input_ids.to(device)
labels = labels.to(device)

# Initialize DDP and ZeroRedundancyOptimizer
ddp_model = DDP(model, device_ids=[torch.cuda.current_device()])
optimizer = torch.optim.AdamW(ddp_model.parameters(), lr=1e-5)
optimizer = ZeroRedundancyOptimizer(ddp_model.parameters(), optimizer)

# Training loop
for epoch in range(num_epochs):
    outputs = ddp_model(**inputs, labels=labels)
    loss = outputs.loss
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
</code></pre>
  </div>


<p>pytorch FSDP offers variety of sharding strategies which is determined by sharding factor F number of ranks over which parameters are
sharded by setting the sharding factor to 1</p>
<h1 id="references">References<a href="#references" class="hanchor" ariaLabel="Anchor">#</a> </h1>
<ul>
<li><a href="https://tinkerd.net/blog/machine-learning/distributed-training/">https://tinkerd.net/blog/machine-learning/distributed-training/</a></li>
<li><a href="https://arxiv.org/abs/2304.11277">PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel</a></li>
<li><a href="https://arxiv.org/abs/2104.07857">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</a></li>
</ul>

      </div></div>

  
    
<div class="pagination">
  <div class="pagination__title">
    <span class="pagination__title-h">Read other posts</span>
    <hr />
  </div>
  <div class="pagination__buttons">
    
      <a href="//localhost:1313/posts/float32/" class="button inline prev">
        dont misuse floating point numbers
      </a>
    
    
      ::
    
    
      <a href="//localhost:1313/posts/torchdistributed/" class="button inline next">
        Distributed Training
      </a>
    
  </div>
</div>


  

  
    

  
</article>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2024 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/bundle.min.js"></script>





  
</div>

</body>
</html>
