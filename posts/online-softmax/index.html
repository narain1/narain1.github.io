<!doctype html><html lang=en><head><title>Online Softmax :: Narain</title>
<meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="One of the most important task in deep learning is classification. This involves predicting the class to which a given input data belongs. Models such as convolution neural networks (CNN) and Large language models use classification layers. These models produce output predictions for all possible classes, but these predictions are not immediately usable as they can be any floating point number. The softmax function is essential in converting these raw model outputs, known as logits, into probabilities that sum to one, making them interpretable"><meta name=keywords content=","><meta name=robots content="noodp"><link rel=canonical href=/posts/online-softmax/><link rel=stylesheet href=/styles.css><link rel="shortcut icon" href=/img/theme-colors/pink.png><link rel=apple-touch-icon href=/img/theme-colors/pink.png><meta name=twitter:card content="summary"><meta name=twitter:site content><meta name=twitter:creator content><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Online Softmax"><meta property="og:description" content="One of the most important task in deep learning is classification. This involves predicting the class to which a given input data belongs. Models such as convolution neural networks (CNN) and Large language models use classification layers. These models produce output predictions for all possible classes, but these predictions are not immediately usable as they can be any floating point number. The softmax function is essential in converting these raw model outputs, known as logits, into probabilities that sum to one, making them interpretable"><meta property="og:url" content="/posts/online-softmax/"><meta property="og:site_name" content="Narain"><meta property="og:image" content="/"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2024-02-26 16:32:46 -0700 MST"></head><body class=pink><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Narain</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=about>About</a></li><li><a href=https://narain1.netlify.app>Showcase</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=about>About</a></li><li><a href=https://narain1.netlify.app>Showcase</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=/posts/online-softmax/>Online Softmax</a></h1><div class=post-meta><time class=post-date>2024-02-26</time></div><span class=post-tags>#<a href=/tags/></a>&nbsp;
#<a href=/tags/></a>&nbsp;</span><div class=post-content><div><p>One of the most important task in deep learning is classification. This involves predicting the class to which
a given input data belongs. Models such as convolution neural networks (CNN) and Large language models use classification layers.
These models produce output predictions for all possible classes, but these predictions are not immediately usable as they can be
any floating point number. The softmax function is essential in converting these raw model outputs, known as logits, into
probabilities that sum to one, making them interpretable</p><h2 id=the-role-of-softmax-in-classification>The role of Softmax in Classification<a href=#the-role-of-softmax-in-classification class=hanchor arialabel=Anchor>&#8983;</a></h2><h3 id=why-softmax>Why softmax?<a href=#why-softmax class=hanchor arialabel=Anchor>&#8983;</a></h3><p>Raw model outputs, or logits can lie anywhere within the floating-point number range, making them difficult to interpret. The
Softmax function addresses this by normalizing the logits into a probability distribution. This allows us to understand the model&rsquo;s
confidence in its predictions.</p><h2 id=steps-to-calculate-sofmax>Steps to calculate Sofmax<a href=#steps-to-calculate-sofmax class=hanchor arialabel=Anchor>&#8983;</a></h2><p><img alt=static src=/posts/online-softmax/images/softmax_default.png></p><ol><li><p>Normalizing by maximum logit:</p><ul><li>The first step involves calculating the maximum value among the model&rsquo;s outputs. This helps in normalizing the outputs
and prevents potential overflow issues during exponentiation.</li></ul></li><li><p>Exponentiation:</p><ul><li>Next we subtract the maximum logit from each logit and calculate the exponent of these normalized values. This step transforms
the logits into a form where they can be compared proportionally.</li></ul></li><li><p>Division by sum of exponents:</p><ul><li>Finally we divide each exponentiated value by the sum of all exponentiated values. This step ensures that the resulting
probabilities sum to one, providing a valid probability distribution.</li></ul></li></ol><h4 id=softmax-implementation-in-python>Softmax implementation in python<a href=#softmax-implementation-in-python class=hanchor arialabel=Anchor>&#8983;</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>softmax</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Compute softmax values for each set of scores in x.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> isinstance(x, torch<span style=color:#f92672>.</span>Tensor):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(x, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    x_max <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>max(dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>values
</span></span><span style=display:flex><span>    e_x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>exp(x <span style=color:#f92672>-</span> x_max)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> e_x <span style=color:#f92672>/</span> e_x<span style=color:#f92672>.</span>sum(dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Example usage</span>
</span></span><span style=display:flex><span>input_tensor <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>2.0</span>, <span style=color:#ae81ff>3.0</span>])
</span></span><span style=display:flex><span>softmax_output <span style=color:#f92672>=</span> softmax(input_tensor)
</span></span><span style=display:flex><span>print(softmax_output)
</span></span></code></pre></div><p>As we can see, the standard Softmax algorithm has some drawbacks. One significant issue is that it requires multiple passes
over the entire tensor to calculate the softmax, makind it less cache-friendly and potentially inefficient.</p><h2 id=introducing-online-softmax>Introducing Online Softmax<a href=#introducing-online-softmax class=hanchor arialabel=Anchor>&#8983;</a></h2><p>To address these inefficiencies, we can explore an alternative approach known as online softmax. This method aims to
improve computational efficiency and cache performance by processing the data in a more streamlined manner.</p><ol><li><p>Single pass for maximum and sum calculation:</p><ul><li>Instead of first finding the maximum value in one pass and then computing the sum of exponentials in another,
this code combines these operation. When a new maximum value is found the sum is adjusted accordingly, ensuring
accuracy without additional passes.</li></ul></li><li><p>Numerical Stability</p><ul><li>By subtracting the maximum value <code>maxval</code> from each element before exponentiation, the code prevents potential overflow
issues that could occur with large input values.</li></ul></li><li><p>Efficiency:</p><ul><li>This approach is more cache-friendly as it reduces the number of passes over the data, thus maximising the number of times the
data needs to be fetched from memory.</li></ul></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;math.h&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;float.h&gt;  // For FLT_MAX</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>void</span> <span style=color:#a6e22e>softmax_forward_online_cpu</span>(<span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> out, <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> inp, <span style=color:#66d9ef>int</span> N, <span style=color:#66d9ef>int</span> C) {
</span></span><span style=display:flex><span>    <span style=color:#75715e>// inp is (N, C)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// out is (N, C), each row of inp will get softmaxed
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Iterate over each row
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> N; i<span style=color:#f92672>++</span>) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> inp_row <span style=color:#f92672>=</span> inp <span style=color:#f92672>+</span> i <span style=color:#f92672>*</span> C; <span style=color:#75715e>// Pointer to the current input row
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> out_row <span style=color:#f92672>=</span> out <span style=color:#f92672>+</span> i <span style=color:#f92672>*</span> C;       <span style=color:#75715e>// Pointer to the current output row
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>float</span> maxval <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>FLT_MAX; <span style=color:#75715e>// Initialize max value to a very small number
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#66d9ef>float</span> sum <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0f</span>;        <span style=color:#75715e>// Initialize sum of exponentials to 0
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>// First pass: find the max value and calculate the sum of exponentials
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> j <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; j <span style=color:#f92672>&lt;</span> C; j<span style=color:#f92672>++</span>) {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>float</span> maxval_prev <span style=color:#f92672>=</span> maxval; <span style=color:#75715e>// Store previous max value
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>            <span style=color:#66d9ef>if</span> (inp_row[j] <span style=color:#f92672>&gt;</span> maxval) {
</span></span><span style=display:flex><span>                <span style=color:#75715e>// Update max value if current element is greater
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>                maxval <span style=color:#f92672>=</span> inp_row[j];
</span></span><span style=display:flex><span>                <span style=color:#75715e>// Adjust the sum with the new max value
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>                sum <span style=color:#f92672>=</span> sum <span style=color:#f92672>*</span> <span style=color:#a6e22e>expf</span>(maxval_prev <span style=color:#f92672>-</span> maxval) <span style=color:#f92672>+</span> <span style=color:#a6e22e>expf</span>(inp_row[j] <span style=color:#f92672>-</span> maxval);
</span></span><span style=display:flex><span>            } <span style=color:#66d9ef>else</span> {
</span></span><span style=display:flex><span>                <span style=color:#75715e>// Update sum if max value does not change
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>                sum <span style=color:#f92672>+=</span> <span style=color:#a6e22e>expf</span>(inp_row[j] <span style=color:#f92672>-</span> maxval);
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>// Second pass: calculate the softmax probabilities
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> j <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; j <span style=color:#f92672>&lt;</span> C; j<span style=color:#f92672>++</span>) {
</span></span><span style=display:flex><span>            out_row[j] <span style=color:#f92672>=</span> <span style=color:#a6e22e>expf</span>(inp_row[j] <span style=color:#f92672>-</span> maxval) <span style=color:#f92672>/</span> sum; <span style=color:#75715e>// Normalize each element
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>This code becomes even more efficient when used on GPU&rsquo;s as all tensor data can be moved to GPU&rsquo;s shared memory</p><h1 id=references>References<a href=#references class=hanchor arialabel=Anchor>&#8983;</a></h1><ul><li><a href=https://arxiv.org/pdf/1805.02867>Online normalizer calculation for softmax Paper</a></li></ul></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button next"><a href=/posts/sentencepiece-tokenizer/><span class=button__text>Sentencepiece Tokenizer</span>
<span class=button__icon>→</span></a></span></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2024 Powered by <a href=https://gohugo.io>Hugo</a></span>
<span>:: <a href=https://github.com/panr/hugo-theme-terminal target=_blank>Theme</a> made by <a href=https://github.com/panr target=_blank>panr</a></span></div></div></footer><script type=text/javascript src=/bundle.min.js></script></div></body></html>