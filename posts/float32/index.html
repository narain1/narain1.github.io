<!doctype html><html lang=en><head><title>Understanding floating point numbers :: Narain</title>
<meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="introduction In 1991, a Patriot missile defense system in Saudi Arabia failed to intercept an incoming Scud missile, resulting in the tragic deaths of 28 American soldiers. The cause? A tiny floating point error that accumulated over time, throwing off the system&amp;rsquo;s timing by just 0.34 seconds. This incident highlights the critical importance of understanding floating point numbers in computer systems. Floating point representation allows computers to work with a wide range of real numbers, from the microscopic to the astronomical."><meta name=keywords content=","><meta name=robots content="noodp"><link rel=canonical href=/posts/float32/><script async src="https://www.googletagmanager.com/gtag/js?id=G-9H0RBE72N6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-9H0RBE72N6")}</script><link rel=stylesheet href=/css/buttons.min.2bc533403a27dfe0e93105a92502b42ce4587e2e4a87d9f7d349e51e16e09478.css><link rel=stylesheet href=/css/code.min.00125962708925857e7b66dbc58391d55be1191a3d0ce2034de8c9cd2c481c36.css><link rel=stylesheet href=/css/fonts.min.90c955c31dd7c0e05aae3d4f583d4d8a2af799d69c961337eaf2a825063a55dd.css><link rel=stylesheet href=/css/footer.min.2e3eb191baee58dd05a9f0104ac1fab0827bca7c64dafe0b2579f934c33a1d69.css><link rel=stylesheet href=/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=/css/header.min.b6fb4423cf82a9f9d7abc9cd010223fa3d70a6526a3f28f8e17d814c06e18f9e.css><link rel=stylesheet href=/css/main.min.1d8be2dd1b5de9fdaed058c8c59fcf4485f36619574abfb47ed0cfda4812c16d.css><link rel=stylesheet href=/css/menu.min.83637a90d903026bc280d3f82f96ceb06c5fc72b7c1a8d686afb5bbf818a29f7.css><link rel=stylesheet href=/css/pagination.min.82f6400eae7c7c6dc3c866733c2ec0579e4089608fea69400ff85b3880aa0d3c.css><link rel=stylesheet href=/css/post.min.fc74ca360273c1d828da3c02b8174eba435607b369d98418ccc6f2243cd4e75d.css><link rel=stylesheet href=/css/prism.min.9023bbc24533d09e97a51a0a42a5a7bfe4c591ae167c5551fb1d2191d11977c0.css><link rel=stylesheet href=/css/syntax.min.cc789ed9377260d7949ea4c18781fc58959a89287210fe4edbff44ebfc1511b6.css><link rel=stylesheet href=/css/terminal.min.736caf886baa67df630c4cde30fbdc5115122eb74c6246f15a31401344bfa2f0.css><link rel=stylesheet href=/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel=stylesheet href=/terminal.css><link rel="shortcut icon" href=/favicon.png><link rel=apple-touch-icon href=/apple-touch-icon.png><meta name=twitter:card content="summary"><meta name=twitter:site content><meta name=twitter:creator content><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Understanding floating point numbers"><meta property="og:description" content="introduction In 1991, a Patriot missile defense system in Saudi Arabia failed to intercept an incoming Scud missile, resulting in the tragic deaths of 28 American soldiers. The cause? A tiny floating point error that accumulated over time, throwing off the system&amp;rsquo;s timing by just 0.34 seconds. This incident highlights the critical importance of understanding floating point numbers in computer systems. Floating point representation allows computers to work with a wide range of real numbers, from the microscopic to the astronomical."><meta property="og:url" content="/posts/float32/"><meta property="og:site_name" content="Narain"><meta property="og:image" content="/"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2024-06-21 16:24:31 -0700 -0700"></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Narain</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/about>About</a></li><li><a href=https://about.narain.dev>portfolio</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/about>About</a></li><li><a href=https://about.narain.dev>portfolio</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=/posts/float32/>Understanding floating point numbers</a></h1><div class=post-meta><time class=post-date>2024-06-21</time></div><span class=post-tags>#<a href=/tags/floats/>floats</a>&nbsp;
#<a href=/tags/></a>&nbsp;</span><div class=post-content><div><h2 id=introduction>introduction<a href=#introduction class=hanchor arialabel=Anchor>#</a></h2><p>In 1991, a Patriot missile defense system in Saudi Arabia failed to intercept an incoming Scud missile,
resulting in the tragic deaths of 28 American soldiers. The cause? A tiny floating point error that
accumulated over time, throwing off the system&rsquo;s timing by just 0.34 seconds. This incident highlights the
critical importance of understanding floating point numbers in computer systems. Floating point representation
allows computers to work with a wide range of real numbers, from the microscopic to the astronomical.
However, it also introduces complexities and potential inaccuracies that programmers must carefully navigate.</p><p>It is important to understand the way we represent floating point numbers in computers to avoid unwanted edge
cases in programming. if you are considering to use float32 for managing your finances just because you would be able
to represent cents be mindful that IEEE 754 skips a number after 2^24 (16,777,217). At this point you are not only missing
numbers but errors due to calculations keeps on adding up.</p><pre tabindex=0><code>Disclaimer Throughout this writeup i will be consistently switching between hardware and its usecases in 
writing applications as we are bound by the hardware design
</code></pre><h3 id=how-is-floating-point-numbers-represented-in-binary>how is floating point numbers represented in binary<a href=#how-is-floating-point-numbers-represented-in-binary class=hanchor arialabel=Anchor>#</a></h3><figure class=center><img src=/images/floatvsint.png alt="Binary Representations"><figcaption class=center>Binary Representations</figcaption></figure><p>sing base-2 (binary) representation for floating point numbers minimizes rounding errors compared to
higher bases. Higher base systems often introduce unused bits, leading to inefficient memory usage.
Furthermore, binary representation aligns with how electronic systems naturally process and store
data, making it the optimal choice for computational tasks</p><p>Real numbers are infinite between any two integers, but representing them on computers with
fixed bit widths involves trade-offs. The density of floating point numbers decays
exponentially as we move away from zero, meaning that more precision is available near
zero and less as values grow larger. Squeezing infinitely many real numbers into a
fixed bit width necessitates approximations. As a result, there are more representable numbers
between -2 and 2 than in other regions of the number line.</p><div class=collapsable-code><input id=1 type=checkbox checked>
<label for=1><span class=collapsable-code__language>c</span>
<span class=collapsable-code__title>floating point numbers skipping</span>
<span class=collapsable-code__toggle data-label-expand=show data-label-collapse=hide></span></label><pre class=language-c><code>
#include &lt;stdio.h&gt;
#include &lt;math.h&gt;

int main() {
  float a = (float)pow(2, 24);
  for (int i = 1; i &lt; 10; i&#43;&#43;) {
    printf(&#34;%d, %f\n&#34;, i, a &#43; i);
  }
}

// 2**24 &#43; 1 = 16777216.000000
// 2**24 &#43; 2 = 16777218.000000
// 2**24 &#43; 3 = 16777220.000000
// 2**24 &#43; 4 = 16777220.000000
// 2**24 &#43; 5 = 16777220.000000
// 2**24 &#43; 6 = 16777222.000000
// 2**24 &#43; 7 = 16777224.000000
// 2**24 &#43; 8 = 16777224.000000
// 2**24 &#43; 9 = 16777224.000000

</code></pre></div><p>From the above example we observe that as numbers get larger the spacing between consecutive
representable numbers increases. The above case at 2^24 floating point format cannot represent every
integer precisely beyond this point.</p><ul><li>2^24 + 1 the floating point precision is insufficient to capture the difference of just 1 due to rounding error</li><li>2^24 + 2 the floating point precision allows 16777218 to be represented and skips over 16777217.</li></ul><p>this phenomenon is called as <code>floating point precision loss</code> at higher magnitudes, more integers are
skipped because the system can&rsquo;t represent every possible number within the limited floating point precision.</p><pre tabindex=0><code>FACT:
javascript uses float64 to represent all numeric data types
</code></pre><h2 id=history-of-floating-point-numbers>History of Floating point numbers<a href=#history-of-floating-point-numbers class=hanchor arialabel=Anchor>#</a></h2><p>Initially, computers only supported integer arithmetic, as it was sufficient for early applications.
However, as computational needs grew, so did the necessity to represent and manipulate
decimal numbers. To address this, programmers began emulating floating-point arithmetic through
complex algorithms, which required more operations than simple integer addition or subtraction.</p><p><a href=https://gist.github.com/narain1/1520a67646d365bd6260914ff5233bec>Float Emulator</a></p><p>Recognizing the inefficiency of these emulations, chip manufacturers started designing dedicated hardware, such
as floating-point registers, to execute floating-point instructions in a single cycle. These
registers adhere to the IEEE 754 standard, which defines the floating-point format with 1 sign
bit, 23 mantissa bits, and 8 exponent bits. Sticking to this standard is critical for
ensuring consistency across platforms, allowing programmers to achieve identical results on different
systems—vital for reliable and predictable computation.</p><h5 id=evolution>Evolution<a href=#evolution class=hanchor arialabel=Anchor>#</a></h5><p>With the rising concern of the <code>memory wall</code> chip manufacturers build custom low precision floating point
arithmatic such as float8, float16, bfloat16 and tensor-float32.</p><p>where float32 is the standard representation</p><h2 id=efficiency-to-high-precision-calculations>Efficiency to high precision calculations<a href=#efficiency-to-high-precision-calculations class=hanchor arialabel=Anchor>#</a></h2><p>Floating-point operations, due to the limitations of their representation, come with some inherent drawbacks.
One key issue is that they are not strictly commutative—meaning a+b≠b+a in certain cases, particularly in
low-precision arithmetic. This non-commutativity increases rounding errors, especially in
embarrassingly parallel applications, where threads process operands out of order.
While floating-point arithmetic typically works well in serial execution, issues arise when code is
parallelized or when the order of operations (like in loops) is changed, leading to slight variations in results.</p><p>Because of these precision issues, it is a common practice not to validate floating-point results by checking
for exact equality. Instead, computations are often considered correct if the results fall within a specified
tolerance range, ensuring that small rounding errors do not invalidate the outcome.</p><h2 id=further-reads>Further reads<a href=#further-reads class=hanchor arialabel=Anchor>#</a></h2><ul><li><a href=https://nhigham.com/2020/05/04/what-is-floating-point-arithmetic/>Whis is floating point arithmetic</a></li><li><a href=https://randomascii.wordpress.com/2013/07/16/floating-point-determinism/>Floating point determinism</a></li><li><a href=https://yosefk.com/blog/consistency-how-to-defeat-the-purpose-of-ieee-floating-point.html>How to defeat the purpose of IEEE floating point</a></li></ul></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><a href=/posts/omp_parallelization/ class="button inline prev">OMP parallelization
</a>::
<a href=/posts/torchdistributed/ class="button inline next">Distributed Training</a></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2025 Powered by <a href=https://gohugo.io>Hugo</a></span>
<span>:: <a href=https://github.com/panr/hugo-theme-terminal target=_blank>Theme</a> made by <a href=https://github.com/panr target=_blank>panr</a></span></div></div></footer><script type=text/javascript src=/bundle.min.js></script></div></body></html>