<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  
    <title>Tensor parallel in pytorch :: Narain</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Tensor operation distributed across gpus" />
<meta name="keywords" content=", " />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="//localhost:1313/posts/tensorparallel/" />


  






  
  
  
  
  
  <link rel="stylesheet" href="//localhost:1313/styles.css">







  <link rel="shortcut icon" href="//localhost:1313/img/theme-colors/pink.png">
  <link rel="apple-touch-icon" href="//localhost:1313/img/theme-colors/pink.png">



<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Tensor parallel in pytorch">
<meta property="og:description" content="Tensor operation distributed across gpus" />
<meta property="og:url" content="//localhost:1313/posts/tensorparallel/" />
<meta property="og:site_name" content="Narain" />

  
  
  <meta property="og:image" content="//localhost:1313/">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">


  <meta property="article:published_time" content="2024-04-07 11:49:20 -0700 MST" />












</head>
<body class="pink">


<div class="container center headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Narain
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="https://narain1.netlify.app">Showcase</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/about" >About</a></li>
        
      
        
          <li><a href="https://narain1.netlify.app" >Showcase</a></li>
        
      
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<article class="post">
  <h1 class="post-title">
    <a href="//localhost:1313/posts/tensorparallel/">Tensor parallel in pytorch</a>
  </h1>
  <div class="post-meta"><time class="post-date">2024-04-07</time><span class="post-author">Narain</span><span class="post-reading-time">2 min read (340 words)</span></div>

  
    <span class="post-tags">
      
      #<a href="//localhost:1313/tags/ml/">ml</a>&nbsp;
      
      #<a href="//localhost:1313/tags/gpu/">gpu</a>&nbsp;
      
    </span>
  
  


  

  <div class="post-content"><div>
        <p>Tensor parallel is a method in distributed computing where a large tensor operation is divided accross multiple gpus. Particularly useful in training large models
unlike model parallelism or pipeline parallism maintains load over all gpus without a separate scheduler or inefficiency in compute time</p>
<p>As of now, PyTorch doesn&rsquo;t support tensors to be directly split across multiple devices. However, as an alternative, we can partition
large tensors and perform operations on them across multiple devices by splitting or tiling tensors. This approach helps reduce the time taken
to complete performance-critical operations.</p>
<h3 id="example-of-simple-matrix-multiplication-on-multiple-gpus">Example of simple matrix multiplication on multiple gpus<a href="#example-of-simple-matrix-multiplication-on-multiple-gpus" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.distributed <span style="color:#66d9ef">as</span> dist
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n_gpus <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>device_count()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize matrices</span>
</span></span><span style="display:flex;"><span>a <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">4096</span>, <span style="color:#ae81ff">4096</span>)  <span style="color:#75715e"># Example matrix A</span>
</span></span><span style="display:flex;"><span>b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">4096</span>, <span style="color:#ae81ff">4096</span>)  <span style="color:#75715e"># Example matrix B</span>
</span></span></code></pre></div><h4 id="split-the-matrix-a-into-two-parts">Split the matrix A into two parts<a href="#split-the-matrix-a-into-two-parts" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<p>by splitting the matrix A into two parts we can multiply them separately in each of the device by dividing compute equally</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>s <span style="color:#f92672">=</span> a<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">assert</span> s[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">%</span>n_gpus <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#34;Tensors must be equally divisible accross gpus&#34;</span>
</span></span><span style="display:flex;"><span>a1, a2 <span style="color:#f92672">=</span> a<span style="color:#f92672">.</span>split(s[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">//</span>n_gpus, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><p>Assume we have two GPUs, GPU0 and GPU1 Move the tensors to the respective GPUs</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>a1_gpu <span style="color:#f92672">=</span> a1<span style="color:#f92672">.</span>cuda(<span style="color:#ae81ff">0</span>)  <span style="color:#75715e"># Move first half of A to GPU0</span>
</span></span><span style="display:flex;"><span>b_gpu0 <span style="color:#f92672">=</span> b<span style="color:#f92672">.</span>cuda(<span style="color:#ae81ff">0</span>)  <span style="color:#75715e"># Move B to GPU0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>a2_gpu <span style="color:#f92672">=</span> a2<span style="color:#f92672">.</span>cuda(<span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># Move second half of A to GPU1</span>
</span></span><span style="display:flex;"><span>b_gpu1 <span style="color:#f92672">=</span> b<span style="color:#f92672">.</span>cuda(<span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># Copy B to GPU1</span>
</span></span></code></pre></div><h4 id="perform-matrix-multiplication-in-parallel">Perform matrix multiplication in parallel<a href="#perform-matrix-multiplication-in-parallel" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>c1_gpu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(a1_gpu, b_gpu0)  <span style="color:#75715e"># Perform on GPU0</span>
</span></span><span style="display:flex;"><span>c2_gpu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(a2_gpu, b_gpu1)  <span style="color:#75715e"># Perform on GPU1</span>
</span></span></code></pre></div><h4 id="combining-the-results">Combining the results<a href="#combining-the-results" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<p>Note: This step requires communication between GPUs which can be done using PyTorch&rsquo;s distributed communication package or manually
Here we&rsquo;ll just simulate the combination by moving the results to CPU and concatenating</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>c1 <span style="color:#f92672">=</span> c1_gpu<span style="color:#f92672">.</span>cpu()
</span></span><span style="display:flex;"><span>c2 <span style="color:#f92672">=</span> c2_gpu<span style="color:#f92672">.</span>cpu()
</span></span><span style="display:flex;"><span>c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((c1, c2), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Result of A x B using tensor parallelism:&#34;</span>, c)
</span></span></code></pre></div><p>Once the operations are induvidually completed in each of the devices the can aggregated onto a single device or can stay on the same device if
further operations are required</p>
<h2 id="useful-links">Useful Links<a href="#useful-links" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<ul>
<li><a href="https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md">Distributed tensor in pytorch</a></li>
</ul>

      </div></div>

  
    
<div class="pagination">
    <div class="pagination__title">
        <span class="pagination__title-h">Read other posts</span>
        <hr />
    </div>
    <div class="pagination__buttons">
        
        <span class="button previous">
            <a href="//localhost:1313/posts/torchdistributed/">
                <span class="button__icon">←</span>
                <span class="button__text">Distributed Training</span>
            </a>
        </span>
        
        
        <span class="button next">
            <a href="//localhost:1313/posts/vectorsearch/">
                <span class="button__text">Vectorsearch</span>
                <span class="button__icon">→</span>
            </a>
        </span>
        
    </div>
</div>

  

  
    

  
</article>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2024 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/bundle.min.js"></script>





  
</div>

</body>
</html>
