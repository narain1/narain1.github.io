<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  
    <title>Distributed training in Pytorch :: Narain</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="dp ddp mp different training regimes how each does it speed up the process what is the difficulty of training regimes that exists now how does it differ from simple distributed computing regime like using mpi as of now dp does not support mp but mp doep
in ddp each gpu is controlled by a worker process
3 important environment variables WORLD_SIZE: defines the total number of workers n1 nodes n2 gpus = n1 * n2 WORLD_RANK: identifies the worker ranges from 0." />
<meta name="keywords" content=", " />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="//localhost:1313/posts/torchdistributed/" />


  






  
  
  
  
  
  <link rel="stylesheet" href="//localhost:1313/styles.css">







  <link rel="shortcut icon" href="//localhost:1313/img/theme-colors/pink.png">
  <link rel="apple-touch-icon" href="//localhost:1313/img/theme-colors/pink.png">



<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Distributed training in Pytorch">
<meta property="og:description" content="dp ddp mp different training regimes how each does it speed up the process what is the difficulty of training regimes that exists now how does it differ from simple distributed computing regime like using mpi as of now dp does not support mp but mp doep
in ddp each gpu is controlled by a worker process
3 important environment variables WORLD_SIZE: defines the total number of workers n1 nodes n2 gpus = n1 * n2 WORLD_RANK: identifies the worker ranges from 0." />
<meta property="og:url" content="//localhost:1313/posts/torchdistributed/" />
<meta property="og:site_name" content="Narain" />

  
  
  <meta property="og:image" content="//localhost:1313/">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">


  <meta property="article:published_time" content="2024-04-07 12:44:46 -0700 MST" />












</head>
<body class="pink">


<div class="container center headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Narain
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/about/">About</a></li>
        
      
        
          <li><a href="https://narain1.netlify.app">Showcase</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/about/" >About</a></li>
        
      
        
          <li><a href="https://narain1.netlify.app" >Showcase</a></li>
        
      
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<article class="post">
  <h1 class="post-title">
    <a href="//localhost:1313/posts/torchdistributed/">Distributed training in Pytorch</a>
  </h1>
  <div class="post-meta"><time class="post-date">2024-04-07</time></div>

  
    <span class="post-tags">
      
      #<a href="//localhost:1313/tags/"></a>&nbsp;
      
      #<a href="//localhost:1313/tags/"></a>&nbsp;
      
    </span>
  
  


  

  <div class="post-content"><div>
        <ul>
<li>
<p>dp ddp mp
different training regimes how each does it speed up the process what is the difficulty of training regimes that exists now
how does it differ from simple distributed computing regime like using mpi
as of now dp does not support mp but mp doep</p>
</li>
<li>
<p>in ddp each gpu is controlled by a worker process</p>
</li>
</ul>
<p>3 important environment variables
WORLD_SIZE: defines the total number of workers n1 nodes n2 gpus = n1 * n2
WORLD_RANK: identifies the worker ranges from 0..(n1*n2)
LOCAL_RANK: defines the id of a worker within a node or indirectly refers to the gpu rank</p>
<ul>
<li>fsdp</li>
</ul>

      </div></div>

  
    
<div class="pagination">
    <div class="pagination__title">
        <span class="pagination__title-h">Read other posts</span>
        <hr />
    </div>
    <div class="pagination__buttons">
        
        <span class="button previous">
            <a href="//localhost:1313/posts/sentencepiece-tokenizer/">
                <span class="button__icon">←</span>
                <span class="button__text">Sentencepiece Tokenizer</span>
            </a>
        </span>
        
        
        <span class="button next">
            <a href="//localhost:1313/posts/tensorparallel/">
                <span class="button__text">Tensor parallel in pytorch</span>
                <span class="button__icon">→</span>
            </a>
        </span>
        
    </div>
</div>

  

  
    

  
</article>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2024 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/bundle.min.js"></script>





  
</div>

</body>
</html>
