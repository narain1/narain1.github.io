<!doctype html><html lang=en><head><title>Synthetic Data Hackathon - AMD :: Narain</title>
<meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Finalist Submission Methodology &amp;amp; Technical Deep Dive First and foremost, I’d like to extend my sincere gratitude to the AMD Hackathon organizers for orchestrating this intensive, high-impact competition especially for investing time in evaluating submissions, facilitating discussions, and reorganizing logistics to ensure fairness and transparency.
I also want to thank all fellow competitors for their thoughtful contributions and active engagement throughout the event particularly those who participated in Discord threads, shared early insights, or offered peer feedback."><meta name=keywords content=","><meta name=robots content="noodp"><link rel=canonical href=/posts/synthetic-data-hackathon/><link rel=stylesheet href=/css/buttons.min.2bc533403a27dfe0e93105a92502b42ce4587e2e4a87d9f7d349e51e16e09478.css><link rel=stylesheet href=/css/code.min.00125962708925857e7b66dbc58391d55be1191a3d0ce2034de8c9cd2c481c36.css><link rel=stylesheet href=/css/fonts.min.90c955c31dd7c0e05aae3d4f583d4d8a2af799d69c961337eaf2a825063a55dd.css><link rel=stylesheet href=/css/footer.min.2e3eb191baee58dd05a9f0104ac1fab0827bca7c64dafe0b2579f934c33a1d69.css><link rel=stylesheet href=/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=/css/header.min.b6fb4423cf82a9f9d7abc9cd010223fa3d70a6526a3f28f8e17d814c06e18f9e.css><link rel=stylesheet href=/css/main.min.1d8be2dd1b5de9fdaed058c8c59fcf4485f36619574abfb47ed0cfda4812c16d.css><link rel=stylesheet href=/css/menu.min.83637a90d903026bc280d3f82f96ceb06c5fc72b7c1a8d686afb5bbf818a29f7.css><link rel=stylesheet href=/css/pagination.min.82f6400eae7c7c6dc3c866733c2ec0579e4089608fea69400ff85b3880aa0d3c.css><link rel=stylesheet href=/css/post.min.fc74ca360273c1d828da3c02b8174eba435607b369d98418ccc6f2243cd4e75d.css><link rel=stylesheet href=/css/prism.min.9023bbc24533d09e97a51a0a42a5a7bfe4c591ae167c5551fb1d2191d11977c0.css><link rel=stylesheet href=/css/syntax.min.cc789ed9377260d7949ea4c18781fc58959a89287210fe4edbff44ebfc1511b6.css><link rel=stylesheet href=/css/terminal.min.736caf886baa67df630c4cde30fbdc5115122eb74c6246f15a31401344bfa2f0.css><link rel=stylesheet href=/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel=stylesheet href=/terminal.css><link rel="shortcut icon" href=/favicon.png><link rel=apple-touch-icon href=/apple-touch-icon.png><meta name=twitter:card content="summary"><meta name=twitter:site content><meta name=twitter:creator content><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Synthetic Data Hackathon - AMD"><meta property="og:description" content="Finalist Submission Methodology &amp;amp; Technical Deep Dive First and foremost, I’d like to extend my sincere gratitude to the AMD Hackathon organizers for orchestrating this intensive, high-impact competition especially for investing time in evaluating submissions, facilitating discussions, and reorganizing logistics to ensure fairness and transparency.
I also want to thank all fellow competitors for their thoughtful contributions and active engagement throughout the event particularly those who participated in Discord threads, shared early insights, or offered peer feedback."><meta property="og:url" content="/posts/synthetic-data-hackathon/"><meta property="og:site_name" content="Narain"><meta property="og:image" content="/"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2025-10-31 17:15:52 +0000 UTC"></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Narain</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/about>About</a></li><li><a href=https://narain1.netlify.app>portfolio</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/about>About</a></li><li><a href=https://narain1.netlify.app>portfolio</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=/posts/synthetic-data-hackathon/>Synthetic Data Hackathon - AMD</a></h1><div class=post-meta><time class=post-date>2025-10-31</time><span class=post-author>Narain, Ram</span></div><span class=post-tags>#<a href=/tags/hackathon/>Hackathon</a>&nbsp;
#<a href=/tags/ai/>AI</a>&nbsp;</span><div class=post-content><div><h1 id=finalist-submission-methodology--technical-deep-dive>Finalist Submission Methodology & Technical Deep Dive<a href=#finalist-submission-methodology--technical-deep-dive class=hanchor arialabel=Anchor>#</a></h1><p>First and foremost, I’d like to extend my sincere gratitude to the AMD Hackathon organizers for orchestrating this intensive, high-impact competition especially for investing time in evaluating submissions, facilitating discussions, and reorganizing logistics to ensure fairness and transparency.</p><p>I also want to thank all fellow competitors for their thoughtful contributions and active engagement throughout the event particularly those who participated in Discord threads, shared early insights, or offered peer feedback. Your collaboration elevated the entire experience.</p><p>Special thanks to EDA Danial and Sanyam for dedicating significant time to evaluate submissions, provide nuanced feedback, and help realign judging frameworks as needed.</p><h2 id=problem-context-why-synthetic-data>Problem Context: Why Synthetic Data?<a href=#problem-context-why-synthetic-data class=hanchor arialabel=Anchor>#</a></h2><p>The core challenge of this hackathon centered around generating high-quality, domain-relevant synthetic datasets to train or fine-tune LLMs specifically for reasoning tasks involving character relationships and seating arrangement (linear and circular).</p><p>Initial approaches involved Synthetic data generation involving a assorted set of aptitude like questions from <a href=https://huggingface.co/datasets/nvidia/OpenMathReasoning>nvidia/OpenMathReasoning</a> .</p><p>Early attempts at brute-force data generation using generic Aptitude datasets (including math, logic puzzles, and general knowledge Q&amp;A) failed to produce usable training material because:</p><ul><li>The semantic relationships between entities were either absent or inconsistent.</li><li>Outputs suffered from heavy repetition due to over-reliance on a single base model without diversification strategies.</li><li>Generated samples lacked contextual grounding making them unsuitable for fine-tuning models intended for relational reasoning.</li></ul><p>This led us to abandon “scrape-and-prompt” approaches and instead design a <strong>multi-stage, relationship-aware synthetic data pipeline</strong> grounded in structured generation, guided prompting, and iterative refinement.</p><h2 id=methodology-multi-stage-question-answer-pair-generation-pipeline>Methodology: Multi-Stage Question-Answer Pair Generation Pipeline<a href=#methodology-multi-stage-question-answer-pair-generation-pipeline class=hanchor arialabel=Anchor>#</a></h2><p>Prior to embarking on synthetic data generation, we dedicated substantial effort to curating a robust initial dataset. We recognized that a high-quality foundation maximizes training efficiency and long-term returns. Unsloth&rsquo;s integration was pivotal throughout the hackathon, enabling seamless workflows without NaN errors—issues typically plaguing fp16 and quantized setups.</p><p>We designed a three-phase framework to generate synthetic QA pairs that encode meaningful relationships between characters and plot elements mimicking the kind of reasoning required in story-based or logic-driven tasks.</p><h3 id=phase-1-relationship-definition-layer>Phase 1: Relationship Definition Layer<a href=#phase-1-relationship-definition-layer class=hanchor arialabel=Anchor>#</a></h3><p>Before generating any questions, we defined <strong>semantic relationships</strong> between key characters from selected narratives (e.g., <em>Alice in Wonderland</em>). For example:</p><blockquote><p>“Who is the mother of the White Rabbit?” → Answer: <em>Not explicitly stated</em>, but we can infer based on narrative context or assign fictional roles for training purposes.</p></blockquote><p>We mapped out relationships such as:</p><ul><li>Parent-child</li><li>GrandParent-GrandChild</li><li>Niece-nephew</li><li>In-laws</li></ul><p>These relationships formed the backbone of our prompt engineering strategy.</p><h3 id=phase-2-character-assignment--prompt-structuring>Phase 2: Character Assignment & Prompt Structuring<a href=#phase-2-character-assignment--prompt-structuring class=hanchor arialabel=Anchor>#</a></h3><p>Once relationships were defined, we assigned specific names to each role. Example:</p><p><code>Alice</code>, <code>White Rabbit</code>, <code>Cheshire Cat</code>, <code>Mad Hatter</code>, <code>March Hare</code>.</p><p>Each question was then framed around these roles, ensuring consistency across generations. Prompts included explicit instructions like:</p><blockquote><p>“Generate a multiple-choice question where the correct answer reflects the relationship between [Character A] and [Character B], based on their established roles in the story involving the path of Character C, E, D.”</p></blockquote><p>This helped reduce hallucination and increased alignment with expected outputs.</p><h3 id=phase-3-guided-llm-inference-for-relational-reasoning>Phase 3: Guided LLM Inference for Relational Reasoning<a href=#phase-3-guided-llm-inference-for-relational-reasoning class=hanchor arialabel=Anchor>#</a></h3><p>Instead of letting the LLM freely generate answers, we used <strong>constrained generation techniques</strong>:</p><ul><li>We provided the LLM with both the question AND the correct answer, asking it to explain <em>why</em> that answer is correct based on character relationships.</li><li>Then, we reversed the process: gave only the question + relationship context, and asked the LLM to generate plausible distractors (wrong answers) that still made sense narratively — increasing dataset diversity without sacrificing relevance.</li></ul><p>This two-way validation loop significantly improved output quality and reduced noise.</p><h4 id=one-of-the-largely-found-exploit-in-arrangement-was-sorting>One of the largely found exploit in arrangement was sorting<a href=#one-of-the-largely-found-exploit-in-arrangement-was-sorting class=hanchor arialabel=Anchor>#</a></h4><blockquote><p>Thanks to the tokenisation of Language models.</p></blockquote><p>A prominent vulnerability in seating arrangement tasks (linear or circular) involves alphabetical sorting of entity names. LLMs frequently err by defaulting to sorted orders when given random names, leading to incorrect inferences. We excluded such patterns from our training data to avoid propagating these flaws, as generated answers often replicated the errors.</p><h2 id=technical-implementation-details>Technical Implementation details<a href=#technical-implementation-details class=hanchor arialabel=Anchor>#</a></h2><h3 id=model-selection--training-setup>Model Selection & Training Setup<a href=#model-selection--training-setup class=hanchor arialabel=Anchor>#</a></h3><p>We experimented with several open-source LLMs known for strong reasoning capabilities:</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Llama 2 70B</td><td>70B parameters</td></tr><tr><td>Qwen3-Next 80B MoE</td><td>80B MoE</td></tr><tr><td>Phi-4</td><td>Smaller, efficient</td></tr><tr><td>R1-based LLMs</td><td>Custom fine-tuned variants</td></tr></tbody></table><p>All models were run locally or via cloud APIs with controlled temperature settings (typically 0.3–0.7) to balance creativity vs. accuracy.</p><h3 id=structured-output-grammar-enforcement>Structured Output (grammar) Enforcement<a href=#structured-output-grammar-enforcement class=hanchor arialabel=Anchor>#</a></h3><p>One critical breakthrough was enforcing <strong>structured output formats</strong> during generation. Instead of free-form text, we required LLMs to return JSON-like structures containing:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span><span style=color:#f92672>&#34;question&#34;</span>: <span style=color:#e6db74>&#34;What is the relationship between Alice and the Cheshire Cat?&#34;</span>,
</span></span><span style=display:flex><span><span style=color:#f92672>&#34;answer&#34;</span>: <span style=color:#e6db74>&#34;Observer/Advisor&#34;</span>,
</span></span><span style=display:flex><span><span style=color:#f92672>&#34;distractors&#34;</span>: [<span style=color:#e6db74>&#34;Mother&#34;</span>, <span style=color:#e6db74>&#34;Enemy&#34;</span>, <span style=color:#e6db74>&#34;Student&#34;</span>],
</span></span><span style=display:flex><span><span style=color:#f92672>&#34;relationship_type&#34;</span>: <span style=color:#e6db74>&#34;narrative_role&#34;</span>,
</span></span><span style=display:flex><span><span style=color:#f92672>&#34;context_hint&#34;</span>: <span style=color:#e6db74>&#34;The Cheshire Cat provides cryptic advice but does not directly influence Alice&#39;s actions.&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>This enabled automated parsing, deduplication, and downstream integration into training pipelines.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;topic&#34;</span>: <span style=color:#e6db74>&#34;TopicEnum.FAMILY_TREE&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;question&#34;</span>: <span style=color:#e6db74>&#34;Symbolic notation: A + B → A is mother of B; A – B → A is brother of B; A % B → A is father of B; A × B → A is sister of B. Which of the following shows that P is the maternal uncle of Q?, answer=C&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;reasoning&#34;</span>: <span style=color:#e6db74>&#34;P – M → P is brother of M; M + N → M is mother of N; N × Q → N is sister of Q. So, P is brother of Q’s mother → maternal uncle.&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;options&#34;</span>: [<span style=color:#e6db74>&#34;A. Q – N + M × P&#34;</span>, <span style=color:#e6db74>&#34;B. P + S × N – Q&#34;</span>, <span style=color:#e6db74>&#34;C. P – M + N × Q&#34;</span>, <span style=color:#e6db74>&#34;D. Q – S % P&#34;</span>]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=question-generation-model>Question Generation model<a href=#question-generation-model class=hanchor arialabel=Anchor>#</a></h2><p>// constraints</p><blockquote><p>max_token_limit = 1024
timelimit for generation = 10 seconds ~100 tokens/s</p></blockquote><h2 id=modeling-strategy>Modeling Strategy<a href=#modeling-strategy class=hanchor arialabel=Anchor>#</a></h2><h3 id=initial-training-phase-sft-with-system-prompts>Initial Training Phase: SFT with System Prompts<a href=#initial-training-phase-sft-with-system-prompts class=hanchor arialabel=Anchor>#</a></h3><p>We began by fine-tuning base LLMs (Llama 3 70B, Qwen3-Next 80B MoE) using <strong>system prompts similar to starter notebooks</strong> :</p><blockquote><p>“You are a logic puzzle generator. Your task is to create one multiple-choice question per output. The question must involve symbolic relationships between entities (e.g., A + B → A is mother of B). Include a clear reasoning trace explaining how to derive the correct answer. Return output in strict JSON format.”</p></blockquote><p>We also added <strong>format enforcement instructions</strong> such as:</p><blockquote><p>“Always include exactly four options labeled A, B, C, D. Never omit any keys. Do not use markdown or plain text only valid JSON.”</p></blockquote><p>This reduced early-stage hallucinations and format violations.</p><h3 id=grpo-reward-functions>GRPO Reward Functions<a href=#grpo-reward-functions class=hanchor arialabel=Anchor>#</a></h3><p>To overcome issues with output format and missing parts of the questions, we applied <strong>Guided Reinforcement Policy Optimization (GRPO)</strong> with custom rewards:</p><p>1.<strong>JSON Format Compliance</strong>
2.<strong>Presence of All Required Keys</strong> - Enforces inclusion of<code>topic</code>,<code>question</code>,<code>reasoning</code>,<code>options</code>
3.<strong>Single-Character Answer Enum (A/B/C/D)</strong> -Prevents invalid answer labels like “Option C” or “C.”</p><p>These rewards were weighted during reinforcement learning phases to nudge the model toward clean, machine-readable outputs.</p><h3 id=what-didnt-work>What Didn’t Work<a href=#what-didnt-work class=hanchor arialabel=Anchor>#</a></h3><p>Despite initial success, we encountered critical issues:</p><h4 id=repetitive-outputs-due-to-static-system-prompts>Repetitive Outputs Due to Static System Prompts<a href=#repetitive-outputs-due-to-static-system-prompts class=hanchor arialabel=Anchor>#</a></h4><p>Even after SFT, the model would generate near-identical questions repeatedly because the system prompt remained constant across generations.</p><h4 id=flawed-questions-with-invalid-logic>Flawed Questions With Invalid Logic<a href=#flawed-questions-with-invalid-logic class=hanchor arialabel=Anchor>#</a></h4><p>Some generated questions had internal contradictions or unsolvable logic (e.g., “If A is brother of B and B is sister of C, then A is sister of C” false!).</p><h4 id=unfinished-experimentation-cross-validation-via-answer-model>Unfinished Experimentation: Cross-Validation via Answer Model<a href=#unfinished-experimentation-cross-validation-via-answer-model class=hanchor arialabel=Anchor>#</a></h4><p>Started with the idea of exploring using the <strong>trained Answer Generation Model</strong> as a <strong>validator</strong>:</p><blockquote><p>For every generated question, we passed it to the Answer Model and checked:</p><ul><li>Does it return a valid JSON?</li><li>Is the answer among A/B/C/D?</li><li>Does the reasoning match the expected logic?</li></ul></blockquote><p>This could creaties a <strong>self-correcting pipeline</strong> if the Answer Model couldn’t solve the question, the Question Model was penalized during GRPO updates.</p><p>This significantly improved logical consistency and reduced flawed samples.</p><h2 id=answer-generation-model>Answer Generation model<a href=#answer-generation-model class=hanchor arialabel=Anchor>#</a></h2><p>// constraints</p><blockquote><p>max_token_limit = 512
time_limit = 5 seconds ~100 tokens per second</p></blockquote><h1 id=answer-generation-model-1>ANSWER GENERATION MODEL<a href=#answer-generation-model-1 class=hanchor arialabel=Anchor>#</a></h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#75715e>// constraints
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>{
</span></span><span style=display:flex><span><span style=color:#f92672>&#34;max_token_limit&#34;</span>: <span style=color:#ae81ff>512</span>,
</span></span><span style=display:flex><span><span style=color:#f92672>&#34;time_limit&#34;</span>: <span style=color:#e6db74>&#34;5 seconds (~100 tokens/sec)&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=modeling-strategy-1>Modeling Strategy<a href=#modeling-strategy-1 class=hanchor arialabel=Anchor>#</a></h2><h3 id=intuition-behind-design>Intuition Behind Design<a href=#intuition-behind-design class=hanchor arialabel=Anchor>#</a></h3><p>Rather than treating this as a simple “answer extraction” task, we modeled it as a <strong>reasoning-first, answer-second</strong> process mimicking human problem-solving:</p><blockquote><p>“First understand the logic. Then deduce the answer. Only then present it clearly.”</p></blockquote><h3 id=supervised-fine-tuning-sft>Supervised Fine-Tuning (SFT)<a href=#supervised-fine-tuning-sft class=hanchor arialabel=Anchor>#</a></h3><p>Trained on curated <strong>question-answer-reasoning triplets</strong> from our own dataset.</p><p>Each sample followed this schema:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span><span style=color:#f92672>&#34;question&#34;</span>: <span style=color:#e6db74>&#34;...&#34;</span>,
</span></span><span style=display:flex><span><span style=color:#f92672>&#34;reasoning&#34;</span>: <span style=color:#e6db74>&#34;... step-by-step derivation ...&#34;</span>,
</span></span><span style=display:flex><span><span style=color:#f92672>&#34;answer&#34;</span>: <span style=color:#e6db74>&#34;C&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>We had two training phases here</p><ol><li>longer reasoning context</li><li>Shorter reasoning context</li></ol><p>The shorter reasoning helps us in reducing the token output.</p><h2 id=grpo-reward-functions-1>GRPO Reward Functions<a href=#grpo-reward-functions-1 class=hanchor arialabel=Anchor>#</a></h2><p>Applied during iterative tuning to refine behavior beyond basic accuracy:</p><table><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td>1.<strong>Exact JSON Pattern Match</strong></td><td>1.0</td><td>Must match predefined schema exactly</td></tr><tr><td>2.<strong>Correct Answer Letter</strong></td><td>2.0</td><td>Highest weight — getting the right choice matters most</td></tr><tr><td>3.<strong>Approximate JSON Pattern</strong></td><td>0.5</td><td>Tolerates minor formatting deviations if meaning preserved</td></tr><tr><td>4.<strong>Presence of All Expected Keys</strong></td><td>0.1 per key</td><td>Encourages completeness without over-penalizing</td></tr></tbody></table><p>This weighting strategy prioritized <strong>correctness > structure > completeness</strong>, which aligned with our goal: build a reliable validator for the Question Generator (TODO).</p><h2 id=why-reasoning-matters-more-than-just-answer>Why Reasoning Matters More Than Just Answer<a href=#why-reasoning-matters-more-than-just-answer class=hanchor arialabel=Anchor>#</a></h2><p>We found that <strong>the reasoning field is crucial for labeling correctness</strong>.</p><p>Example:</p><blockquote><p>Question: “Who is P’s maternal uncle?”<br>Option C: “P – M + N × Q”<br>Reasoning: “P – M → P is brother of M; M + N → M is mother of N; N × Q → N is sister of Q. So P is brother of Q’s mother → maternal uncle.”</p></blockquote><p>Without the reasoning, you can’t verify whether option C is truly correct — especially when symbolic notation varies.</p><p>Thus, we treated reasoning as a <strong>core component of ground truth</strong>, not just an explanation.</p><h2 id=what-didnt-work-1>What Didn’t Work<a href=#what-didnt-work-1 class=hanchor arialabel=Anchor>#</a></h2><h3 id=rewarding-low-token-usage-in-reasoning>Rewarding Low Token Usage in Reasoning<a href=#rewarding-low-token-usage-in-reasoning class=hanchor arialabel=Anchor>#</a></h3><p>We initially tried rewarding shorter reasoning traces to encourage efficiency.</p><p>Failed because:</p><ul><li>Often led to skipped steps or unjustified conclusions</li><li>Reduced model confidence and increased error rates - removed this reward entirely. Instead, focused on <strong>logical completeness</strong> and <strong>clarity of inference chain</strong>.</li></ul><h2 id=summary>Summary<a href=#summary class=hanchor arialabel=Anchor>#</a></h2><p>For final submissions, the question model used a base, unfine-tuned Phi-4 14B, while the answering model employed Qwen3 14B with LoRA training via SFT and GRPO. To ensure compatibility and fairness, we adhered closely to the provided submission code templates, minimizing deviations.</p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><a href=/posts/llm-inference-on-lambda/ class="button inline next">Llm Inference on Lambda</a></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2025 Powered by <a href=https://gohugo.io>Hugo</a></span>
<span>:: <a href=https://github.com/panr/hugo-theme-terminal target=_blank>Theme</a> made by <a href=https://github.com/panr target=_blank>panr</a></span></div></div></footer><script type=text/javascript src=/bundle.min.js></script></div></body></html>